{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Thinking about the Problem\n",
      "We need informed image comparison, meaning that the methods we use are sensitive to the transformation of the data, and to an extent, the goals of the researcher.  The first \"transformation\" to tackle is image thresholding. It is not clear what the \"best\" strategy is to calculate something like a voxelwise pearson correlation given that an image has been thresholded.\n",
      "\n",
      "## What does it mean to threshold an image?\n",
      "Thresholding means setting values below some image intensity value, \"threshold,\" to 0, which introduces 0's into an image in areas that previously had data. We do this (I think) because it leaves \"the most significant\" parts of statistical maps that people like to look at as colorful blobs on anatomical brains.  Thresholding is not done for the data, it is done for the person.\n",
      "\n",
      "## What does a zero in an image indicate?\n",
      "Zeros and NaN's *should* indicate that the voxel in question was not inside a brain mask.  It follows, then, that when we threshold, we are artifically removing voxels from the brain mask.  And the poor analysts that must deal with our metaphorical killing of data must either only use the smaller set that remains, or introduce \"faux\" zeros. Both of these cases make me feel very squirmy. There really is no good reason to threshold other than to appeal to the desires of human visual cortex.\n",
      "\n",
      "## How can things go wrong?\n",
      "To motivate the problem, I will think of a few case examples of how thresholding can make things \"go wrong.\" In both these cases, we have a map that has been severely thresholded, as has mostly zeros:\n",
      "\n",
      "### Case 1: Pairwise Deletion - We do image comparison with only non-zero, non-nan values:\n",
      "This is what we must do if we are not confident if thresholding has been done.  This comes down to taking the intersection of non-zero, non-nan voxels in two images, and calculating the correlation between that set.  What this will do is bias the image comparison to return small masks.  Tal summarizes this well:\n",
      "\n",
      "> I would be very wary of using only the intersection of non-zero voxels to compute the correlation between two maps without having a small-volume penalty of some kind. What's going to happen is that the variance of correlations is going to be huge for very small regions, which means that it's very likely that once you have a couple of thousand images, the top N correlations with a whole-brain map will be dominated by small masks. This will lead people to make exactly the wrong kind of inference, and will actually reward them for uploading maps with less information.\n",
      "\n",
      "I believe this is what Chris is calling \"Pairwise deletion,\" and under the case that we disable image comparison for thresholded maps, this is a reasonable approach to take (while we are figuring out a better soluion).\n",
      "\n",
      "### Case 2: Brain Mask - We do image comprison with all values within a brain mask:\n",
      "This is what we would want to do with an unthresholded image. In an unthresholded image, we can be confident of our zeros. If the image has been thresholded, then again we are treating faux zeros as real zeros, and this may lead to strangeness. This would also be equivalent to setting nan values to zero and then treating them as real zeros.\n",
      "\n",
      "### Case 3: Pairwise Inclusion - We do image comprison with all voxels that are not zero, not-nan, in either map\n",
      "This means summing two images together, and using all voxels that finite / non-zero values.  This means that we could have a zero in one image, but if it's a finite value in the other, we would use it. This strategy would not have the same bias to return smaller maps if a comparison was done between an unthresholded image and a smaller one, because the zeros would be included in the smaller one.  On the other hand, the comparison would possibly be biased to better match smaller images to other smaller images, because of those zeros. You could imagine taking an unthresholded map, thresholding, and then comparing the thresholded version to its previously meaty self and some other (equivalently sized but different) thresholded map, and I'd guess the two images that are thresholded would be regarded as more similar. Chris mentions: \n",
      "\n",
      "> Replacing missing values with zeroes (or any other arbitrary number) is not a good solution to this problem and leads to different sort of biases. It actually increases the estimation error of the correlation coefficient, by creating a systematic bias.\n",
      "\n",
      "### A Temporary Workaround\n",
      "Now we are talking about NeuroVault. While we push for users to upload *unthresholded* maps in MNI space, this cannot be guaranteed.  As stated above, in the meantime, we need to stress to users that thresholded maps are dangerous. Joe is working on an additional to NeuroVault to warn the user of this, and we have [discussed](https://github.com/NeuroVault/NeuroVault/pull/111#issuecomment-72343860) not including images that have been thresholded in similarity calculations.  As a temporary comparison metric, we will use \"pairwise deletion,\" defined by Chris as calculating:\n",
      "\n",
      "> a non-zero and non-nan mask for each image and using their intersection to calculate correlation.\n",
      "    \n",
      "    \n",
      "# Goals\n",
      "\n",
      "   - We shouldn't need to completely outlaw thresholded images from image comparison. We need to properly deal with them.\n",
      "   - We want to test how different transformations (thresholds from unthresholded --> coordinate-esche) and different similarity metrics perform to match things together that should be matched together. \n",
      "   - We want a similarity metric that can appropriately penalise small masks (ROIs) so the search is not biased to return smaller (thresholded) images.\n",
      "   - Likely the first experiments will include the basic ideas discussed above, however from this I am hopeful of developing a more successful \"penalty strategy.\"\n",
      "\n",
      "\n",
      "# Questions\n",
      "I like to approach problems by trying to ask questions. So let's do that.\n",
      "\n",
      "## High Level Questions\n",
      " - How does the same image compare to itself at different transformations? How do we deal with zeros/nans when we compare two brain maps? Are there metrics better suited to different challenges?\n",
      "  - **Part I**: How does the same image compare to itself at different thresholds?\n",
      "  - **Part II**: How does the same image compare to itself using different similarity metrics?\n",
      "  \n",
      "## Specific Questions Brainstorm\n",
      "  \n",
      " - **Pairwise deletion**: given a large set (>1000) images, what happens to the variance of correlations? _Tal hypothesizes it will be huge for small regions, small for larger ones_.\n",
      "  - **A**: How does a small, medium, large ROI compare to (some) same set of unthresholded maps?\n",
      "  - **B**: How does an unthresholded map compare to different sets of thresholded maps? _Tal hypothesizes \"the top N correlations with a whole-brain map will be dominated by small masks\"_\n",
      " - **Pairwise inclusion**: How does replacing missing values with zero (in one map) bias the comparison? _Chris hypothesizes that it will increase estimation error of correlation coefficient by creating a systematic bias_\n",
      " - **Brain Mask**: How does replacing missing values with zero (in both maps) bias the comparison?\n",
      " - Is there a metric that is more robust to these challenges (eg, for the above studies, test the same thing on a set of reasonable / interesting similarity metrics)\n",
      " - **ROI Mask**: Is is common enough practice using an ROI to segment a statistical map to your area of interest (eg, in SPM) to include ROI approaches as one of the \"kinds\" of thresholding?\n",
      " - **Similarity Metrics** Which ones are better suited to the challenges above?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Setting up Experiments\n",
      "I now want to think about how to address the questions above.\n",
      "\n",
      "## Preparation:\n",
      "Learn about similarity metrics and \"corrections,\" and write functions to easily implement for brain images. I should have enough understanding to have expectations for the metrics in the experiments detailed below.\n",
      "\n",
      "## Experiment 1: \n",
      "Individual images compared to transformations of themselves.\n",
      "\n",
      " - Define \"gold standard\" as a comparison of image A with itself (using brain mask I assume?).\n",
      " - Define a range of thresholds to apply to image (from unthresholded --> coordinate) (ROIs too?)\n",
      " - Define a set of similarity metrics to use\n",
      " - For each of pairwise comparison, pairwise inclusion, and brain mask, and for each similarity metric, test how scores change at the different levels of threshold.\n",
      " \n",
      "## Experiment 2: \n",
      "Individual image (at same range of thresholds) compared to large set of unthresholded maps (including its previous self).\n",
      "\n",
      " - Define \"gold standard\" as unthresholded image compared to itself and others\n",
      "   - This vector of scores can be used to calculate error\n",
      " - Can we label (group) images based on study, and get a sense if a particular thresholding strategy maybe makes the map less similar to its previous self, but more similar to other studies measuring \"the same thing\" ? Couldn't that be beneficial? \n",
      " - Again define a range of thresholds to apply to image (from unthresholded --> coordinate) (ROIs too?)\n",
      " - Again define a set of similarity metrics to use (better informed by experiment 1?)\n",
      " - Label images based on study, and thresholding level, so beyond calculating  \n",
      "\n",
      "## Experiment 3: \n",
      "Set of images (at same range of thresholds) compared to all other images in set at different thresholds.\n",
      "\n",
      " - For this approach, we likely need some kind of \"scoring\" algorithm. We would *want* an image, regardless of the threshold, to always be most closely matched to itself. On the other hand, we should give some points if it's matched to another image in its same study. We also want to know if images of the same thresholding level are better matched to one another just because of the thresholding. So...\n",
      "\n",
      "### How do we assess how well we did?\n",
      "We do it based on ordering.  The gold standard is again the distance of unthresholded maps, or with this \"labeling\" approach, the ascending ordered list of similar images. For example, let's say that we have three images, A B and C:\n",
      " \n",
      "    A\n",
      "    B\n",
      "    C\n",
      " \n",
      "We could then calculate the \"gold standard\" similarity, an ordering for each map:\n",
      "\n",
      "| |A |B |C |\n",
      "|-:|-:|-:|-:|\n",
      "|1| A|B |C |\n",
      "|2| C|C |A |\n",
      "|3| B|A |B |\n",
      "\n",
      "Now if we are to do two levels of thresholding for every single map, we would have this set of images:\n",
      "\n",
      "\n",
      "    A, A1, B, B1, C, C1\n",
      "    \n",
      "(Before I was thinking of *nan* bread, and now I am thinking of steak sauce!)\n",
      "\n",
      "Now let's look at the list of image A. If image C is originally ranked number 2 when compared to image A (at the unthresholded \"gold standard\" level) then we would be quite happy if C and C1 were both still \"around that level.\" Matter of fact, if our algorithm was all knowing and powerful he would be immune to any thresholding and would *know* that a new comparison result should look like this:\n",
      "\n",
      "| |A |B |C |\n",
      "|-:|-:|-:|-:|\n",
      "|1| A|B |C |\n",
      "|2| A1|B1|C1 |\n",
      "|3| C|C |A |\n",
      "|4|C1|C1 |A1 |\n",
      "|5| B|A |B |\n",
      "|6| B1|A1|B1 |\n",
      "\n",
      "\n",
      "So my idea is to define a \"scoring\" metric to assess the result that \"gets at\" how things move around in this list.  We give full points if A or any of its derivatives are in the first two slots, and subtract points the further away we go from those spots. \n",
      "\n",
      "### Take transformation and study into account\n",
      "The approach above will (I think) take the study into account, because the unthresholded maps from the same study are going to be more similar to one another.  However, we need a way to also take transformation into account.  For example, if I want to find maps similar to transformation X, it could be that I would want to optimize getting maps that are of the same transformation. So I propose:\n",
      "\n",
      "- For each image, transform it in all ways\n",
      "- Label transformed images with their original id, study, and the level of thresholding.\n",
      "- Define \"gold standard\" (the ordered list for each image)\n",
      "- Transform the ordered list into a set of \"acceptable\" ranges for each image. \n",
      "- Apply similarity metrics / image comparisons!\n",
      "- For each similarity metric, calculate mean/sd of scores/errors for:\n",
      "   - the entire list of images\n",
      "   - images compared to different transformations of themselves\n",
      "   - images compared to all different transformations, \"within study\"\n",
      "   - images compared to each specific transformation\n",
      "   - images compared to each specific transformation, \"within study\"\n",
      "   \n",
      "Each of the above bullets (I think) somewhat gets at different goals that researchers might have when they do comparison. We obviously want images to be matched to their original selves, but we also want to understand comparison on the level of study, and in fact, we may want to add another labeling for the experiment type (eg, emotional vs risk task). The ideal outcome of this would be some kind of lookup table where the researcher can choose an optimal strategy based on the composition of the database (transformations) and what he/she wants to find (everything, study, experiment type, etc.)\n",
      "\n",
      "# Plan of Action\n",
      "I am going to start with learning (eg, reading papers and the internet), and writing functions. I won't feel comfortable setting my expectations if my statistical knowledge doesn't extend beyond a very tiny set of similarity metrics! Then I will do Experiment 1, which doesn't require a complicated dataset beyond a small set of unthresholded maps. Then we can move forward with figuring out some of the more complicated experiments!"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}