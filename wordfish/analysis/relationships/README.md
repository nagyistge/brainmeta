Each of these pdfs shows a matrix of relationships, and the relationship between some term1 and term2 is derived as the similarity value of the word embeddings between the two terms. The word embeddings are generated by word2vec via a neural network.

Format of files is as follows:

[corpus_model]_[terminology]_[board]

- corpus model: is the entire text corpus from which word embeddings were derived
- terminology: describes the set of terms that filtered the corpus. Note that to simplify this, I just used single terms. Models can be derived with phrases, but I haven't tried it yet.
- board: In the case of the corpus being "reddit" - I could train a model on just a subset (a particular board). These relationship matrices were derived from a subset of the corpus just from that board.
