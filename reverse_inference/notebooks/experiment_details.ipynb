{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Does Image Similarity Map Onto Map Similarity?\n",
      "\n",
      " - *Experiment 1*: code all of the tasks in terms of an ontology (*cough* cognitive atlas) and then (to) assess similarity using a metric that is aware of the ontological similarity of the contrasts.  \n",
      " - *Experiment 2*: do the inverse - basically to use the confusions to cluster the contrasts, and then to examine the clustering of tasks in order to assess how well they fit the ontology.\n",
      "     \n",
      "     \n",
      "### Overview of Plan\n",
      "\n",
      " - We want to combine HCP with Openfmri.\n",
      " - [Finish coding](https://github.com/vsoch/taskfmri) HCP and Openfmri contrasts, meaning mapping contrasts to concepts in Openfmri\n",
      " - Add all Openfmri tasks to the the cognitive atlas (if not existing)\n",
      " - Get feedback on mappings\n",
      " - Make concept --> contrast assertions\n",
      " - Get new RDF dumps\n",
      " - Then do experiments\n",
      " \n",
      "Since all task data for both HCP and Openfmri is in NeuroVault, I'd like to do the analysis in an \"as programmatic as possible\" way so as we get more tagged maps in NeuroVault, it will be easy to reproduce. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Methods to Assess Ontological Similarity\n",
      "(aka, how similar are these brain images based on where they fall in the ontology?)\n",
      "\n",
      "We want to assess the ontological similarity of brain statistical maps. Why? When we use different tasks and contrasts to measure cognitive phenomena, there can be multiple ways of making assertions about the resulting inferences that can be made, meaning that different descriptions can actually be describing very similar (or even the same) thing. If each of those inferences was represented as a static text label (eg, \"emotion\"), we would not be able to assess similarity to another text label (eg, \"anger\") because we don't know how the two terms are related.  This is the strength of ontology - ontology defines entities (such as these text labels) and in addition, relationships between them (\"anger\" is a kind of \"emotion\"). This means that we can classify different brain statistical maps onto nodes (entities) in the ontology (instances) and then assess similarity of the maps based on the relationships, this \"ontological similarity.\" How do we do this? We have methods to assess ontological similarity, and there are (broadly) 5 kinds. I want to briefly discuss each, as well as include implementations that I could use.\n",
      "\n",
      "### Methods based on semantic distance\n",
      "\n",
      "Semantic distance: some function of distance between two terms on the ontology tree. Terms that are closer together are more similar. If there are multiple paths, we can use the shortest or average path. There are several things we have to take into consideration when doing this calculation:\n",
      "\n",
      " - **density of the graph** because more dense graphs will have shorter overall distanes.\n",
      " - **depth** because deeper nodes without \"sideways\" relationships will be less similar\n",
      " - **link type** which basically comes down to the weight of the relationship.\n",
      "\n",
      "### Key Methods: \n",
      "#### shortest path\n",
      "     \n",
      "     R. Rada, H. Mili, E. Bicknell, and M. Blettner, \u201cDevelopment and application of a metric on semantic nets,\u201d IEEE Transactions on Systems, Man and Cybernetics, vol. 19, pp. 17\u201330, 1989.\n",
      "\n",
      "     Similarity(c1,c2) = 2*(maximum length path) - (shortest path between c1,c2)\n",
      "\n",
      " - Pros: simple and good for \"isa\" relationships\n",
      " - Con: doesn't account for different kinds of relationships, and semantic relatedness represented in edges\n",
      "\n",
      "#### connection weight\n",
      "     M. Sussna, \u201cWord sense disambiguation for free-text indexing using a massive semantic network,\u201d in Proceedings of the 2nd International Conference on Information and Knowledge Management, pp. 67\u201374, ACM, Washington, DC, USA, November 1993. View at Scopus\n",
      "\n",
      "[sussna]\n",
      "\n",
      "This equation determines an edge weight (wt)\n",
      "     -->r is a relationship\n",
      "     -->r' is its inverse\n",
      "     d: depth of deeper node\n",
      "     maxr,minr: max and min weights for a relation of type r\n",
      "     n(r)x: the number of relations of type r leaving a node x\n",
      "\n",
      "#### common path technique\n",
      "took into account the position of c1 and c2 relative to nearest common ancestor (c).\n",
      "\n",
      "![commpath](http://www.vbmis.com/bmi/share/notebook/reverse_inference/commpath.png)\n",
      "\n",
      "D1 and D2 are the shortest paths from c1 and c2 to c and H the shortest from c to the root. \n",
      "\n",
      " - con: could lose semantics of edges... But all edges must be same weight.\n",
      "\n",
      "nodes at the same depth don't necessarily have same specificity\n",
      "relationships (edges) at same level don't necessarily indicate the same senmantic distance\n",
      "\n",
      "     Wu and Palmer Z. Wu and M. Palmer, \u201cVerbs semantics and lexical selection,\u201d in Proceedings of the 32nd Annual Meeting on Association for Computational Linguistics, pp. 133\u2013138, Association for Computational Linguistics, Las Cruces, NM, USA, 1994.\n",
      "\n",
      "\n",
      "### Methods based on information content\n",
      "Similarity is determined based on Information Content (IC) of the lowest common ancestor. IC tells us how specific and informative a term is, and is the negative log of the likelihood, P(c), the probability of occurance of c in a specific corpus.  In \"brain map\" terms, that (probably) means the probability of occurance of a concept (term) for a given set of brain maps.  IC can also be computed based on the number of children a node has (it says this approach is less commonly used, but since we have less data, this might be more ideal for our purpose).\n",
      "\n",
      " - Con: methods based on IC may be inaccurate due to shallow annotations. (this kind of feels like CA, although we will want to look at the tree after additions are added).\n",
      "\n",
      "![resnik](http://www.vbmis.com/bmi/share/notebook/reverse_inference/resnik.png)\n",
      "\n",
      "\n",
      "this measure is based on information content - the negative algorithm of the probability of its occurrence and the similarity between terms c1 and c2.\n",
      "\n",
      "S(c1,c2) is the set of all parents for c1 and c2. This metric is symmetric and transitive.\n",
      "\n",
      " - Con: only suitable for ontologies with one kind of relationship, because the node at the top has a similarity of 1.\n",
      "\n",
      "     \n",
      "     P. Resnik, \u201cSemantic similarity in a taxonomy: an information-based measure and its application to problems of ambiguity in natural language,\u201d Journal of Artificial Intelligence Research, vol. 11, pp. 95\u2013130, 1999. View at Google Scholar \u00b7 View at Scopus\n",
      "\n",
      "Lin makes the following assumptions:\n",
      "\n",
      " - similarity is associated with common properties. More of those == more similar\n",
      " -  similarity also associated with differences. more different = lower similarity (redundant)\n",
      " - similarity is maximized when two terms are the same.\n",
      "\n",
      "![lin](http://www.vbmis.com/bmi/share/notebook/reverse_inference/lin.png)\n",
      "\n",
      "\n",
      " - c0 is lowest common ancestor, P(c)i and j are the probability of occurance.\n",
      " - Con: relies on high precision of annotation information.\n",
      "\n",
      "### Methods based on properties of terms\n",
      "[these are not my words]\n",
      ">> \"In feature-matching methods, terms are represented as collections of features, and elementary set operations are applied to estimate semantic similarities between terms.\"\n",
      "\n",
      "A feature-matching model in general consists of three components: distinct features of term A to term B, distinct features of term B to term A, and common features of terms  A and B.\n",
      "\n",
      "This method, for example, is based on set theory.\n",
      "     D1 and D2 correspond to description of sets c1 and c2\n",
      "     || is orthogonality (number in set)\n",
      "     mu is a function that defines relative importance of non-common features.\n",
      "\n",
      "![features](http://www.vbmis.com/bmi/share/notebook/reverse_inference/features.png)\n",
      "\n",
      "\n",
      "#### Methods based on ontology hierarchy\n",
      "An ontology is a graph with directionality, called a DAG (directed acyclical graph). The nodes are terms, and the edges are relationships. There are multiple different kinds of relationships, and for the cognitive atlas, the relationships that we care about are defined in the contrasts: \"is a kind of\" and \"are parts of.\" The users of the ontology define these relationships, and make assertions about different tasks-->contrasts measuring the concepts.  There are a couple strategies that people have taken:\n",
      "\n",
      " - shortest path length between two terms\n",
      " - shortest path to lowest common ancestor\n",
      " - most methods generally were using lengths of shortest paths, depths of nodes, commonalities between terms, semantic contributions of ancestor terms, and others, but a weakness of these methods is not taking into account multiple least common ancestor nodes.\n",
      "\n",
      "[encoding a GO terms semantics](http://bioinformatics.oxfordjournals.org/content/23/10/1274.full) was first done by Wang et al, and then the linked author. This could be a potential approach for cognitive atlas because they also use is-a and part-of relationships.\n",
      "\n",
      "[Ontology Structure-based Similarity](http://www.aaai.org/Papers/IJCAI/2007/IJCAI07-087.pdf) is a similarity metric for ontologies defined by Schickel Zuber and Faltings. They wanted the similarity to be a score between [0,1], and should satisfy the following assumptions:\n",
      "\n",
      " - should depend on features of the terms\n",
      " - each feature should contribute independently to a score\n",
      " - unliked or unknown features should not contribute\n",
      "\n",
      "However the highest \"overall correlation with human judgments\" was for [this paper](http://www.sciencedirect.com/science/article/pii/S1532046410001346)\n",
      "\n",
      "\n",
      "### Hybrids\n",
      "\n",
      "As implied, these methods combine things like attribute similarity, ontology hierarchy, information content, and the depth of the LCA node.\n",
      "\n",
      "### Software!\n",
      "\n",
      "Here is the important part! There are several software for deriving ontology based similarity:\n",
      "\n",
      "#### [GOSemSim](http://bioinformatics.oxfordjournals.org/content/26/7/976.short) on [bioconductor](http://bioconductor.org/packages/2.6/bioc/html/GOSemSim.html)\n",
      " - is an R package\n",
      " - using other databases is listed as a con, but this is a non-issue, because I would implement for cognitive atlas\n",
      "\n",
      "#### [seGosa](http://ieeexplore.ieee.org.ezproxy.stanford.edu/xpls/icp.jsp?arnumber=5706624)\n",
      "\n",
      " - I contacted the main author to get the software\n",
      " - implemented in java\n",
      " - uses information theoretic approaches\n",
      " - gives two approaches for assessing similiarity\n",
      " - has many successful applications with GO\n",
      "\n",
      "#### [DOSim](http://www.biomedcentral.com/1471-2105/12/266)\n",
      "[and download](http://210.46.85.150/platform/dosim/)\n",
      "\n",
      " - incorporates enrichment analysis\n",
      " - \"multi-layered\" enrichment analysis\n",
      " - has been applied successfully\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Work Log\n",
      "\n",
      "#### 4/18/2015\n",
      "\n",
      "- OpenfMRI contrasts from Chris updated from Z-->T maps (newer pipeline) in NeuroVault\n",
      "- pyneurovault api updated for new neurosynth and neurovault api\n",
      "- NeuroVault API modified to expose openfmri task uid\n",
      "- extraction of [openfmri and hcp image data](https://github.com/NeuroVault/pyneurovault/blob/master/examples/download_example.py) (and tagged tasks) from NeuroVault programmatically\n",
      "- preliminary work to [define concepts for each contrast](https://github.com/vsoch/taskfmri/blob/master/all_contrasts_cognitive_atlas_coding.tsv)\n",
      "\n",
      "\n",
      "#### 4/19/2015\n",
      "\n",
      "- Finished \"guesstimating\" concepts for each contrast\n",
      "- Russ: we need to pin down the contrasts for [ds115](http://neurovault.org/collections/428/) I found the conditions but not contrasts on TACC. I think someone named \"Deanna\" might know, as I've seen her name in a few places.\n",
      "\n",
      "#### 4/23/2015\n",
      "\n",
      "- Had meeting to discuss contrast tagging, were able to select subset of images to be tagged.\n",
      " - Updated sheet to only include selected subset\n",
      " - Will meet again early May after first pass at tagging to discuss\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}