{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# How do we compare things?\n",
      "We use similarity metrics! Let's learn about them. First, the data.\n",
      "\n",
      "### Whole-brain statistical maps\n",
      "A brain map is a 3D image, however for our purposes we can squash it down and just represent it as a vector of numbers.  As stated nicely by Tom, we are \"seeking a test of independence between 2 high dimensional vectors X & Y, where each X & Y are correlated.\"\n",
      "\n",
      "#### Why are X & Y correlated?\n",
      "I think there are two levels of correlation here.  There is going to be within-map spatial correlation, and between-map correlation. The first doesn't need much explanation - voxels near one another are likely similar in function / structure and have the same blood vessels flowing away from them to contribute to the BOLD signal. Voxels between maps that are correlated could be do to a similar study, brain, or experiment. \n",
      "\n",
      "#### Start with reading\n",
      "I will to read the following:\n",
      "\n",
      "1. Shevlyakov, G., & Smirnov, P. (2011). [Robust estimation of a correlation coefficient: an attempt of survey.](http://www.stat.tugraz.at/AJS/ausg111+2/111+2Shevlyakov.pdf) Australian & New Zealand Journal of Statistics, 40(1), 147\u2013156. \n",
      "\n",
      "2. Kharin, Y. S., & Voloshko, V. a. (2011). [Robust estimation of AR coefficients under simultaneously influencing outliers and missing values.](doi:10.1016/j.jspi.2011.04.015) Journal of Statistical Planning and Inference, 141(9), 3276\u20133288. \n",
      "\n",
      "3. Roche, A., Malandain, G., & Ayache, N. (2000). [Unifying Maximum Likelihood Approaches in Medical Image Registration.](http://www3.interscience.wiley.com/journal/72501437/abstract ) International Journal of Imaging Systems and Technology, 11(1), 71\u201380."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vanessa,\n",
      "\n",
      "Happy to help out.  As I see it, you are seeking a test of independence between 2 high dimensional vectors X & Y, where each X & Y are correlated.  In full generality, there's no need for X & Y to have the same dimension, and you're trying to assess the non-zero-ness of the nVoxelX x nVoxelY cross-correlation matrix.  Of course, with only one observation (i.e. in computing the similarity of contrast X and contrast Y, you don't don't have multiple realisations of X & Y to help you), you can never hope to estimate the cross-correlation matrix and you're left working with a single (bivariate) correlation computed on a common set of voxels.\n",
      "\n",
      "As far as correlation computations go, you're out of the standard setting because your bivariate samples (i.e. pairs (X_i,Y_i), over voxels i) are dependent over space.  That's what my email below to Jeanette concerns (her X & Y were vectors over time, but the issues are identical).  So, roughly, you're trying to compute sample correlation r = X'Y / n (assuming X & Y are standardised), while C_X and C_Y express strong spatial structure (i.e. smoothness).  Miraculously, even under dependence, r is unbiased for the true correlation rho, but the variance depends on the degree of smoothing (tr(C_X C_Y)/n^2 to be precise).\n",
      "\n",
      "So... as for what to read?  I'm not sure of your background.  I found these notes useful to get a grip on the sampling distribution of r under usual (independence samples) setting, and sections 1 & 2 of Brown & Rutemillers (1977) for the dependent samples case.  For robust correlation, check out [1,2].\n",
      "\n",
      "Otherwise, you're in the general world of image similarity metrics; you might find this treatise useful [3.  It shows how the various different image similarity metrics correspond to different likelihood models for the relationship between the pair of images (to be registered, in their case).   For you, Affine, Functional and Statistical relationships (see the paper) all seem reasonable candidates for you (and justify correlation, correlation ratio, and mutual information, respectively).\n",
      "\n",
      "Enjoy!\n",
      "\n",
      "-Tom\n",
      "\n",
      "1. Shevlyakov, G., & Smirnov, P. (2011). Robust estimation of a correlation coefficient: an attempt of survey. Australian & New Zealand Journal of Statistics, 40(1), 147\u2013156. Retrieved from http://www.stat.tugraz.at/AJS/ausg111+2/111+2Shevlyakov.pdf\n",
      "\n",
      "2. Kharin, Y. S., & Voloshko, V. a. (2011). Robust estimation of AR coefficients under simultaneously influencing outliers and missing values. Journal of Statistical Planning and Inference, 141(9), 3276\u20133288. doi:10.1016/j.jspi.2011.04.015\n",
      "\n",
      "3. Roche, A., Malandain, G., & Ayache, N. (2000). Unifying Maximum Likelihood Approaches in Medical Image Registration. International Journal of Imaging Systems and Technology, 11(1), 71\u201380. Retrieved from http://www3.interscience.wiley.com/journal/72501437/abstract \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hi Chris,\n",
      "\n",
      "Ah yes - using MAD would be an interesting idea. There are many cool problems and challenges with comparing different types of images. The one we discussed the most is how to compare two images that have been masked with different masked. The two options we discussed were: (1) compare only voxels in the intersection of the two masks (2) leave the zeros outside the masks and treat the two images as unmasked. Both have problems and biases.\n",
      "\n",
      "To me, #2 seems like a non-starter... Turning \"no data\" into 0 seems really arbitrary. If you assume \"missingness at random\" then using the intersection should be fine. \n",
      "\n",
      "One thing to consider is the special nature of zeros. Eg imagine two studies that have identical z maps but one is shifted by +1 relative to other. Statically these are quite different but would have a perfect correlation.  Or, consider a more pathological situation, where the intersection mask only hits high T's in each image but *both* images have high T's (this would correspond to some stupid case of the mask coming from a thresholding operation, but still...).  As the correlation subtracts the mean, you might find low correlation, when in fact it should be highly compelling that both have high T's (far from zero).\n",
      "\n",
      "So!  While you leave the warm comfort of correlation and its interpretation, I'd consider correlation without centering.  With no centering the dot product of the standardised values is the cosine of the angle between the two images. (Again, just computed on the intersection mask).\n",
      " \n",
      "There are other interesting problems. How to you compare a binary ROI mask with an unthresholded map?\n",
      "\n",
      "Assuming this is truly binary, i.e., have no idea about the intensity, consider this reasoning for a likelihood ratio test:\n",
      "H0 model - Whole (unthresholded) map is mean zero, unit variance\n",
      "H1 model - Map is mean zero, unit variance outside the ROI mask, non-zero inside the ROI mask (but no other constraint).\n",
      "Under a working model of independence (ha! but, wait), the zero-ROI-voxels values will cancel out and the test will reduce to a sum of squares of tz/-test values inside the ROI mask.  *If* you had spatial independence this would be a simple chi^2 test.  But if you make a typical Gaussian smoothness assumption *and* you know the FWHM of the noise, you can Satterthwaite that puppy and get an approximate chi^2.\n",
      "\n",
      "Bartlett's correction\n",
      "How do you compare a parcellation map with an unthresholded map?\n",
      "\n",
      "Very tricky, as you need to have some belief that the T/Z values for the parcels means the same thing as the unthresholded map.  Just thinking aloud, two obvious approaches are: (1) Average T's within parcels, compute a correlation (or angle cosine) parcelwise; (2) voxellate the parcellation, and compute a correlation (or angle cosine) voxel-wise.  While these both should give unbiased estimates of rho, they will differ in their uncertainty (i.e. effective DF).  \n",
      "\n",
      "In fact, the miraculous thing is that getting an unbiased estimate of a correlation coefficient in the presence of (spatial) dependence isn't so hard.  But if you need a *precision* of that estimate, or want to r2z it, then you're in trouble.\n",
      "\n",
      "I'll forward a detailed discussion I had with Jeannette about this last summer... it might be useful for Vanessa to pour over if she'll be knee deep in correlations based on correlated data.\n",
      "\n",
      "-Tom "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tom has stated some things that I need to digest:\n",
      "\n",
      "- In full generality, there's no need for X & Y to have the same dimension, and you're trying to assess the non-zero-ness of the nVoxelX x nVoxelY cross-correlation matrix.  \n",
      "\n",
      "\n",
      "Of course, with only one observation (i.e. in computing the similarity of contrast X and contrast Y, you don't don't have multiple realisations of X & Y to help you), you can never hope to estimate the cross-correlation matrix and you're left working with a single (bivariate) correlation computed on a common set of voxels.\n",
      "\n",
      "As far as correlation computations go, you're out of the standard setting because your bivariate samples (i.e. pairs (X_i,Y_i), over voxels i) are dependent over space.  That's what my email below to Jeanette concerns (her X & Y were vectors over time, but the issues are identical).  So, roughly, you're trying to compute sample correlation r = X'Y / n (assuming X & Y are standardised), while C_X and C_Y express strong spatial structure (i.e. smoothness).  Miraculously, even under dependence, r is unbiased for the true correlation rho, but the variance depends on the degree of smoothing (tr(C_X C_Y)/n^2 to be precise).\n",
      "\n",
      "So... as for what to read?  I'm not sure of your background.  I found these notes useful to get a grip on the sampling distribution of r under usual (independence samples) setting, and sections 1 & 2 of Brown & Rutemillers (1977) for the dependent samples case.  For robust correlation, check out [1,2].\n",
      "\n",
      "uOtherwise, you're in the general world of image similarity metrics; you might find this treatise useful [3.  It shows how the various different image similarity metrics correspond to different likelihood models for the relationship between the pair of images (to be registered, in their case).   For you, Affine, Functional and Statistical relationships (see the paper) all seem reasonable candidates for you (and justify correlation, correlation ratio, and mutual information, respectively).\n",
      "\n",
      "Enjoy!\n",
      "\n",
      "-Tom\n",
      "\n",
      "\n",
      "Jeanette,\n",
      "\n",
      "I have just been pulling my hair out to discover the right reference for \"Bartlett's correction\" for a correlation coefficient, used when the individual samples contributing to the calculation are themselves correlated.  I first read the term in the Van Dijk et al. (2010) paper, which just has a cryptic reference to a time series textbook.  They do describe it as the sum of the squared temporal autocorrelation function.  \n",
      "\n",
      "After digging and digging, I finally found a concise statement of there the result needed.  The set up I'm using is this:  w.l.o.g., assume that X & Y are random vectors, with mean zero and unit variance, but are correlated, i.e. Cov(X)=Cor(X)=C_X, Cov(Y)=Cor(Y)=C_Y.  We want to estimate the correlation between X & Y, Corr(X_i,Y_i), accounting for their 'intra' correlations, C_X & C_Y.  Importantly, there *is* some sort of full joint correlation, Cor(X,Y) = C_XY, but we are only interested in the diagonal of C_XY, which assume is a common value rho.\n",
      "\n",
      "In this setting, the correlation coefficient calculation is \n",
      "   r = X'Y / n\n",
      "where n is the length of the vectors.  By the results in Brown & Rutemillers (1997), under the *null* case when C_XY=0,\n",
      "   E(r) = rho\n",
      "   V(r) = tr(C_X C_Y)/n^2\n",
      "If you assume C_X = C_Y = C and corresponds to a stationary time series, then, you can show that *excepting edge effects*, tr(CC) = nK, where K is the sum of squared elements of one row of C (which, by stationarity, one row looks just like a shifted version of another, again, excepting edge effects).  This gives rise to the \"\"Bartlett's correction\" used in Van Dijk (2010).\n",
      "\n",
      "But *this* shows how to use the result if you have different autocorrelation structures for X & Y.\n",
      "\n",
      "Another interesting thing that I discovered from looking at that 1977 paper is that if the null *is* *not* true (C_XY \\neq 0), then the variance of r dependends on tr(C_XY C_XY)... i.e. *not* just on the diagonal of C_XY but the full joint distribution of the two r.v.s.\n",
      "\n",
      "\n",
      "OK!!!  That's probably way more than you wanted to hear, but I just had to get that off my chest.\n",
      "\n",
      "-Tom\n",
      "\n",
      "Van Dijk, K. R. A., Hedden, T., Venkataraman, A., Evans, K. C., Lazar, S. W., & Buckner, R. L. (2010). Intrinsic functional connectivity as a tool for human connectomics: theory, properties, and optimization. Journal of Neurophysiology, 103(1), 297\u2013321. doi:10.1152/jn.00783.2009\n",
      "\n",
      "Brown, G. G., & Rutemillers, H. C. (1977). Means and variances of stochastic vector products with applications to random linear models. Management Science, 24(2), 210\u2013216. Retrieved from https://calhoun.nps.edu/public/bitstream/handle/10945/29964/meansvariancesof00brow.pdf?sequence=1  or see PDF\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}